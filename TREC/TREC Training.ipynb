{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from gensim.models import word2vec\n",
    "import random\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, LSTM, Bidirectional, GlobalMaxPooling1D, Lambda, Add, Dot\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(fname):\n",
    "    f = open(fname, \"rb\")\n",
    "    result = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgLength(q_doc, p_doc, n_doc):\n",
    "    \n",
    "    avg_q = 0\n",
    "    avg_p = 0\n",
    "    avg_n = 0\n",
    "    \n",
    "    for i in range(len(q_doc)):\n",
    "        avg_q = avg_q + len(q_doc[i])\n",
    "        avg_p = avg_p + len(p_doc[i])\n",
    "        avg_n = avg_n + len(n_doc[i])\n",
    "    \n",
    "    print(avg_q / len(q_doc))\n",
    "    print(avg_p / len(q_doc))\n",
    "    print(avg_n / len(q_doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pattern = re.compile('[a-zA-Z]+')\n",
    "    return (match.group(0) for match in pattern.finditer(text))\n",
    "\n",
    "def makeTrainFeatuers(trec_train, vec_model, word_index):\n",
    "    \n",
    "    q_docs = []\n",
    "    p_docs = []\n",
    "    n_docs = []\n",
    "    for e in trec_train:\n",
    "        tmp_q_docs = []\n",
    "        tmp_p_docs = []\n",
    "        tmp_n_docs = []\n",
    "        \n",
    "        q = list(tokenize(e[0]))\n",
    "        \n",
    "        p = e[1]['p']\n",
    "        p = p[random.randrange(len(p))]\n",
    "        p = list(tokenize(p))\n",
    "        \n",
    "        n = e[1]['n']\n",
    "        n = n[random.randrange(len(n))]\n",
    "        n = list(tokenize(n))\n",
    "        \n",
    "        for token in q:\n",
    "            if (token in vec_model.wv.vocab):\n",
    "                \n",
    "                tmp_q_docs.append(word_index[token])\n",
    "                \n",
    "        for token in p:\n",
    "            if (token in vec_model.wv.vocab):\n",
    "                tmp_p_docs.append(word_index[token])\n",
    "                    \n",
    "        for token in n:\n",
    "            if (token in vec_model.wv.vocab):\n",
    "                tmp_n_docs.append(word_index[token])  \n",
    "                \n",
    "        q_docs.append(tmp_q_docs)\n",
    "        p_docs.append(tmp_p_docs)\n",
    "        n_docs.append(tmp_n_docs)\n",
    "        \n",
    "    return q_docs, p_docs, n_docs    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(q_docs, p_docs, n_docs):\n",
    "    \n",
    "    max_length = 40\n",
    "    \n",
    "    q_pad = pad_sequences(q_docs, maxlen=max_length, padding='post')\n",
    "    p_pad = pad_sequences(p_docs, maxlen=max_length, padding='post')\n",
    "    n_pad = pad_sequences(n_docs, maxlen=max_length, padding='post')\n",
    "    \n",
    "    return q_pad, p_pad, n_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(vec_model):\n",
    "    max_len = 40 #model的部份主要按照ANSWER SELECTION GITHUB 40是跟者上面的設定\n",
    "    # Input layer\n",
    "    input_q = Input((max_len,))\n",
    "    input_p = Input((max_len,))\n",
    "    input_n = Input((max_len,))\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_matrix = np.zeros((len(vec_model.wv.vocab) + 1, 250)) #250是WORD2VEC訓練的詞向量維度\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = vec_model.wv[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    embedding_layer = Embedding(len(vec_model.wv.vocab) + 1, 250, weights=[embedding_matrix], input_length=40)\n",
    "    \n",
    "    emb_q = embedding_layer(input_q)\n",
    "    emb_p = embedding_layer(input_p)\n",
    "    emb_n = embedding_layer(input_n)\n",
    "    \n",
    "    # Bi-lstm layer\n",
    "    shared_lstm = Bidirectional(\n",
    "        LSTM(300, return_sequences=True)#RETUREN SEQUENCE回傳所有時間點的隱藏曾的直\n",
    "    )\n",
    "    encode_q = shared_lstm(emb_q)\n",
    "    encode_p = shared_lstm(emb_p)\n",
    "    encode_n = shared_lstm(emb_n)\n",
    "    \n",
    "    # Max pooling layer\n",
    "    vec_q = GlobalMaxPooling1D()(encode_q)\n",
    "    vec_p = GlobalMaxPooling1D()(encode_p)\n",
    "    vec_n = GlobalMaxPooling1D()(encode_n)\n",
    "    \n",
    "    cosine_p = Dot(axes=1, normalize=True)([vec_q, vec_p])\n",
    "    cosine_n = Dot(axes=1, normalize=True)([vec_q, vec_n])\n",
    "    \n",
    "    neg = Lambda(lambda x: -x)\n",
    "    sub = Add()([cosine_n, neg(cosine_p)])#這邊開始的LOSS 主要按照論文李的HINGE LOSS 可以看一下我有沒有寫錯\n",
    "    \n",
    "    def hinge(x):\n",
    "        tmp = K.zeros(K.shape(x))\n",
    "        return K.maximum(tmp, 0.1 + x) #錯誤的答案跟正確的至少可以相差0.2 再大就被視為一樣好 \n",
    "    \n",
    "    hinge1 = Lambda(hinge)\n",
    "    result = hinge1(sub)\n",
    "    \n",
    "    model = Model(inputs=[input_q, input_p, input_n], outputs=[result, cosine_p, cosine_n])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_train = readFile(\"trec_train.pkl\")\n",
    "word_index = readFile(\"word_index_dic.pkl\")\n",
    "vec_model = word2vec.Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "q_docs, p_docs, n_docs = makeTrainFeatuers(trec_train, vec_model, word_index)\n",
    "q_pad, p_pad, n_pad = pad(q_docs, p_docs, n_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8098106712564546\n",
      "16.195352839931154\n",
      "13.333046471600689\n"
     ]
    }
   ],
   "source": [
    "avgLength(q_docs, p_docs, n_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = buildModel(vec_model)\n",
    "model.compile(\n",
    "    optimizer='adam', loss='mean_squared_error', loss_weights=[1, 0, 0]#只看HINGE LOSS 未來可以加入權重\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 40, 250)      3898000     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 40, 600)      1322400     embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "                                                                 embedding_1[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 600)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 600)          0           bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 600)          0           bidirectional_1[2][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1)            0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1)            0           dot_2[0][0]                      \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1)            0           add_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 5,220,400\n",
      "Trainable params: 5,220,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label1 = np.zeros((len(q_pad), 1)) # 這邊地一個是HINGE LOSS 地2個是cos true 3 is cos neg\n",
    "label2 = np.ones((len(q_pad), 1)) # cos 正確的最大是1\n",
    "label3 = np.ones((len(q_pad), 1))\n",
    "label3 = label3 * -1 # cos錯誤的最好是-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"weights_best_2.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, save_weights_only=False, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1162/1162 [==============================] - 5s 5ms/step - loss: 0.0069 - lambda_2_loss: 0.0069 - dot_1_loss: 0.0134 - dot_2_loss: 3.4446\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.00690, saving model to weights_best_2.hdf5\n",
      "Epoch 2/10\n",
      "1162/1162 [==============================] - 4s 3ms/step - loss: 0.0044 - lambda_2_loss: 0.0044 - dot_1_loss: 0.0128 - dot_2_loss: 3.3763\n",
      "\n",
      "Epoch 00002: loss improved from 0.00690 to 0.00442, saving model to weights_best_2.hdf5\n",
      "Epoch 3/10\n",
      "1162/1162 [==============================] - 4s 3ms/step - loss: 0.0028 - lambda_2_loss: 0.0028 - dot_1_loss: 0.0193 - dot_2_loss: 3.1667\n",
      "\n",
      "Epoch 00003: loss improved from 0.00442 to 0.00283, saving model to weights_best_2.hdf5\n",
      "Epoch 4/10\n",
      "1162/1162 [==============================] - 4s 3ms/step - loss: 0.0015 - lambda_2_loss: 0.0015 - dot_1_loss: 0.0287 - dot_2_loss: 2.9567\n",
      "\n",
      "Epoch 00004: loss improved from 0.00283 to 0.00154, saving model to weights_best_2.hdf5\n",
      "Epoch 5/10\n",
      "1162/1162 [==============================] - 4s 3ms/step - loss: 7.0872e-04 - lambda_2_loss: 7.0872e-04 - dot_1_loss: 0.0378 - dot_2_loss: 2.8012\n",
      "\n",
      "Epoch 00005: loss improved from 0.00154 to 0.00071, saving model to weights_best_2.hdf5\n",
      "Epoch 6/10\n",
      "1162/1162 [==============================] - 4s 3ms/step - loss: 2.8486e-04 - lambda_2_loss: 2.8486e-04 - dot_1_loss: 0.0482 - dot_2_loss: 2.6595\n",
      "\n",
      "Epoch 00006: loss improved from 0.00071 to 0.00028, saving model to weights_best_2.hdf5\n",
      "Epoch 7/10\n",
      "1162/1162 [==============================] - 4s 3ms/step - loss: 1.1266e-04 - lambda_2_loss: 1.1266e-04 - dot_1_loss: 0.0607 - dot_2_loss: 2.5256\n",
      "\n",
      "Epoch 00007: loss improved from 0.00028 to 0.00011, saving model to weights_best_2.hdf5\n",
      "Epoch 8/10\n",
      "1162/1162 [==============================] - 4s 3ms/step - loss: 4.2776e-05 - lambda_2_loss: 4.2776e-05 - dot_1_loss: 0.0706 - dot_2_loss: 2.4366\n",
      "\n",
      "Epoch 00008: loss improved from 0.00011 to 0.00004, saving model to weights_best_2.hdf5\n",
      "Epoch 9/10\n",
      "1162/1162 [==============================] - 4s 3ms/step - loss: 1.7517e-05 - lambda_2_loss: 1.7517e-05 - dot_1_loss: 0.0768 - dot_2_loss: 2.3854\n",
      "\n",
      "Epoch 00009: loss improved from 0.00004 to 0.00002, saving model to weights_best_2.hdf5\n",
      "Epoch 10/10\n",
      "1162/1162 [==============================] - 4s 3ms/step - loss: 9.3784e-06 - lambda_2_loss: 9.3784e-06 - dot_1_loss: 0.0796 - dot_2_loss: 2.3601\n",
      "\n",
      "Epoch 00010: loss improved from 0.00002 to 0.00001, saving model to weights_best_2.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc570e57048>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([q_pad, p_pad, n_pad], [label1, label2, label3], callbacks=callbacks_list, epochs=10, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath=\"weights_best_2.hdf5\" ) # 先把最好的MODEL載入在除存\n",
    "model_json = model.to_json()\n",
    "with open(\"model_margin_0.1_epo_10_2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
